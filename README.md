# Auto-Image-Scraping-Pipeline
Complete pipeline for scraping, reviewing, filtering, and standardizing images.

## Flow
1_scrapping_links.R -> 3_GUI.py -> 4_image_download.py -> 1_parser.py -> csvAnalysis.py -> Crop&Resize.py

## Scraping
**1_scrapping_links.R** is used for scraping. Make sure to run in the native console and not the R console, so you're able to add arguments with the script call.<br/>
### Usage:
Rscript 1_scrapping_links.R \<searchTags> \<immutableIndex> \<instCount> \<wordList><br/>
**searchTags** - tags listed in order of priority, from most to least priority<br/>
**immutableIndex** - index, n; tags between the highest priority to n tag are immutable and will not be removed by the program<br/>
**instCount** - target number of instances per word/query<br/>
**wordList** - CSV holding all words (assumption is made for column name = "English Glossary")<br/>
### Logic:
1. queries generated by concatenating words with available immutable and mutable tags 
2. generated query is then searched using search engine
3. if scraped stalls for 30s then one tag is removed from mutable tags (back to step 1)
4. if all mutable tags are exhausted then it continues the search with the immutable tags
5. if time taken for the current word exceeds 20 mins, then all image addresses collected are saved and script moves onto next word
<br/>Purpose of timeout logic and requerying is to maximize the number of instances scraped while staying within constraints and not losing out on data, while also making sure the scraper doesn't stall out and waste time on any word.

## Reviewing
**3_GUI.py** is used for reviewing. It is a manual process of iterating through each image, and manually approving or rejecting the image. Useful for quality control of the dataset.<br/>
python 3_GUI.py

## Interim Image Download
**4_image_download.py** is used for the temporarily downloading of images. Necessary step to have a copy of the image on the hard drive for later steps that require the dimensions of the image and the actual image itself for cropping.<br/>
python 4_image_download.py \<inputCSV> \<outputFolder><br/>
**inputCSV** - csv holding all image addresses and words<br/>
**outputFolder** - target directory for downloading images<br/>

## Metadata Extraction
**1_parser.py** is used for extracting metadata from images. Surface level data is outputted including, filename, file path, width, height, and resolution. This is an intermediate step to make the following steps cleaner.<br/>
python 1_parser.py \<dirPath><br/>
**dirPath** - path for directory holding all images

## Analysis
**csvAnalysis.py** along with a macro file, createPivot.bas, are used for analysis of the gathered data. Use this analysis for determining which resolution range and diff (aspect ratio) range should be used for your final dataset. Choosing the resolution and diff range allows for better standardization results and decreases the amount of data lost.<br/>
python csvAnalysis.py \<inputCSV> \<outDir> \<resStep><br/>
**inputCSV** - csv containing filename, file path, width, height, and resolution<br/>
**outDir** - output directory for output excel<br/>
**resStep** - size of resolution ranges for output resolution_bin (if resStep = 10; output = 0-9, 10-19)<br/>
The excel file produced contains raw data representing every image. It's suggested to analyze the images, use the tool provided **createPivot.bas**. Simply use the keyboard shortcut ALT + F11 (Windows) within excel where you can drag the macro file in. The macro will produce a pivot table configured to show the number of instances per category, with filters that allow the user to customize the results using diffRange (range in aspect ratio) and resolution_bin (range in resolution). It is suggested that you also select the resolution and category columns from the raw data, and create a box and whisker plot. 

## Standardization
**Crop&Resize.py** is used as a final step for standardization of all images within the given parameters. First crops and then scales up/down to match given target resolution. Cropping before scaling is necessary to omit distortion artifacts that can occur due to the original aspect ratio of the image being disrupted. To specify, images are cropped to be square, essentially cropping to the square of the smaller of the dimensions (width and height).<br/>
python Crop&Resize.py \<xlsxPath> \<diffRange> \<upper/lower> \<numCrops> \<targetDim> \<outputDir><br/>
**xlsxPath** - path to excel file<br/>
**diffRange** - desired diffRange to apply as a filter for which images are downloaded; should've been determined by last step; input as floats within str as such "0.8-1.0"<br/>
**upper/lower** - desired upper and lower limits of resolution to apply as a filter for which images are downloaded; should've been determined by last step; input as int within tuple as such (30000, 39999)<br/>
**numCrops** - number of instances created from each individual image; multiple crops are applied to make sure position of crops are random and therefore fair allowing all detail to be captured and also meant to increase the size of the dataset<br/>
*note: uniqueness of each instance produced varies based on numCrops and starting resolution*<br/>
**targetDim** - target dimension for new square images (only one int representing width and height)<br/>
**outputDir** - ouptut directory for new square images<br/>

###### 4_image_download.py and 3_GUI.py are modified versions of scripts created by Joe Huamani
###### Created with the assistance of AI
###### Uses scripts from other repositories
